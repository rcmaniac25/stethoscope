Format: #<day number>.<attempt number>. <date>

Note: running log of progress and notes that may not be code related. Also, I'm not the best speller.

#0.1. 3/18/2018:
- Project name determined thanks to a friend

#1.1. 3/19/2018:
- Fleshing out docs and making a main function. Plan for the first couple days is to get the project started, do some parsing, how to get some values from logs, and building habits
- I'm not exactly promoting this with the hashtag right now, just to get this going. I don't feel I need motivation, just less stuff to do. I constantly have ideas and write them down, but lack the time to work on it. It makes me easy to distract, so I want to build a habit naturally with everything else I have going on before I start getting "keep going!" tweets and messages.

#2.1. 3/20/2018:
- Almost missed this day (the commit for yesterday and today say they're a day later, but it's because I start late the prior day). #HabitForming
- Already a bit overthinking it because I started trying to do streaming log parsing... just parse the log right now.
- Initial log format is XML. I'm gonna have to make a config file to determine what properties describe the log, just so the log format isn't hardcoded... but that's for a bit later. Maybe tomorrow?
- Initial plan was for a C++ program... but XML parsing, streaming logs, json parsing too (config and I'm sure someone logs in JSON) are not pleasent to work with in C++ and since I want to get something running instead of dealing with frameworks and C++ package systems, I'm just gonna start with C#

#3.1. 3/21/2018:
- I wanted to try and start processing the actual log data, so I finished the XML parsing part. Should still work on making it stream the input. Noted for later.

#4.1. 3/22/2018:
- Late commit again.
- Plan: initial orginization of log output, get streaming XML working (need option parsing so I can turn this on only if needed), have log values determined by config (requires JSON parsing)
- I started late, so I only go the first one done and will do the others tomorrow

#5.1. 3/23/2018:
- If yesterday was late, this one is later...
- I load a config file of info about parsing logs. As it's very late for me, I will test this tomorrow.
- For the "temp" C# project, I decided to be a little curious and use .Net Portable Framework. I'm kinda regretting it. So many APIs just aren't available. I get it's purpose and it should be used, but going 2 levels deep to open a StreamReader? The main library just opened one internally anyway.

#6.1. 3/24/2018:
- Late start because I was finishing something up. Wrote https://github.com/rcmaniac25/setbuilder to help with that task.
- Config for log parsing now used
- Made up a syntax (like XPath) to get out XML data... maybe I should just use XPath? Thoughts for another time
- Changed version, as this isn't 1.0 yet.
- Decided that the XML streaming is a bit too large for doing at this initial phase: why?
-- The log files I initially imagined using this on are in the multi-GB range. This inital impl. loads everything into memory, and loading one or more multi-GB logs would probabaly end poorly.
-- At the same time, I can see two uses of this system (when it reaches 1.0 status):
    1. Tracing across multiple log files.
    2. Manual "debugging" of a log.
-- In both cases, the full logs aren't needed upfront. It's like a sliding window, the system/user is only looking at one part of the log at a time to find/track something.
-- So ideally, the system should load and unload as needed. "Read the first 100 lines of logs. Ok, the user is scrolling, load the next 100. Ok, they got 1000 lines in, load another 100 and unload the first 100. Wait, they did a search... load but don't persist the logs unless it has whatever they searched for".
-- I'd assume info will be cached so it's not constantly parsing. It may be useful to do some tricks, like loading whole sections of the logs into RAM but not parsing them, that way parsing is fast when it is needed.
-- While I'm thinking about some of these things, I will eventually need logging and unit tests. I know I eventually will need to parse source code, if anything, for determining the end of a function and maybe when a function is container within the calling function. Log parsing, code parsing, etc. should all be seperate components so new ones can be used as needed...
-- ...I'm starting to overthink things. Just get some useful parsing done. Tomorrow.

#7.1. 3/25/2018:
- I did not accomplish useful parsing. Instead I refactored code, got a option parser included, and allowed XML parsing to be more "advanced" for each element instead of just the log message itself.
- PSA: Socialize. You'll get good ideas by talking with people about what you're working on. In my case, I didn't do more code (and much of anything else) because I played Fortnite for the whole day. It was fun, especially with friends.

#8.1. 3/26/2018:
- I realized to bring about the idea that's in my head, I need a better way of processing through the logs. This makes me think "database" so I can do queries, but I'm not ready for that point.
-- Basically, gather all the logs together, then do a "get by thread ID" or "get by function" or "get by <random attribute>", and then breaking it down further. Hmm... this sounds like a LINQ query. Fine will we're in .Net, but may be more difficult in C++ (unless I use Rx/Ix)
- This change "broke" the inital printout system I wrote, so I needed to get it back to the inital state it was previously at
- Had a few more log attributes that I can parse. I should look into what other loggers produce/log, so I have a general common set of values and then add some "fancy" parser for supporting log values that aren't common/custom. Also, I still haven't added support for key/value attributes and parsing types of attributes

#9.1. 3/27/2018:
- Orginized the files into folders and added one more parameter. Also fixed a bug where missing parameters could cause the program to crash
- Needed a bit of abstraction for reorginizing sources
- Just realized I finished 10 days of 100 Days of Code. Woot.

#10.1. 3/28/2018:
- Late start, but today's goal was parsing attribute types (and inadvertently formalizing the "path" syntax so I don't have to do string parsing the whole time)
- Needed to touch a little more code then I wanted... but it works, and in theory could be faster then the string parsing from before
- The path system could be expanded... but at that point, it's probably better to revisit the idea of using "paths". Speaking of which, a side reason for formalizing "paths" is because when JSON and other formats are added, we don't want to need special configs for each parser type (though we should support it...)

#11.1. 3/29/2018:
- Added key-value pair support for context attributes, which should simplify things later when we want to trace by a value
- Wasn't working... found out I forgot to actually register the attribute. Yay, copy-paste bugs

#12.1. 3/30/2018:
- Next couple days are with family, so I can't get as much done as I would like...
- Added a printer class so I can support files, console, etc. printouts without needing to rewrite systems

#13.1. 3/31/2018:
- I did one more abstraction to get the parsers and printers from a factory.
- I expect 3 reactions: why? ah, good decision, and premature optimization...
- It's not premature optmization... it's premature generalization. Something of equivilent badness, but only if it's actually going to have an effect.
- In this case, and to awnser the "why", is to shrink the classes down a bit, allow for easy testing with mocks, and to prepare for the inevitable expansion of functionality.
- See, I thought up some ideas on how I want this to operate (I should write them down in some document...), and they all revolved around the GUI. But I know some of the people who have voiced interest are CLI users. Others only use a IDE like Eclipse, VS Code, or Sublime (depending on how you set it up, it can act like an IDE...)
- I can write a single app (that I would still need to move to C++) that has tens of hundreds of options and configs, letting you do anything you wanted... or I can try and be "smart". "What file type are you using? XML? Ok, I should get the XML parser." "I'm a CLI app, so I will print to the console... wait, you specifed you wanted a file, so I will now print to a file"
- How about for "how do I know I didn't break XML parsing by switching from parser X to parser Y?" Easy: tests. But, I need to expose interfaces and internal structures to change values... or I can mock them, abstracted by interfaces, to control within the actual execution class.
- And that darn porting thing. I want this in C++ so instead of requiring *nix users to install C++, or have Java installed... or some other framework. How about "extract a program from the zip, and it runs". Like a good-old portable app.
- Here's what now happens:
-- 1. Bunch of small classes, as independent functionality and decision making is broken up, makes it easier to port eventually. It also allows for making focused classes, such as one that says "what do we need to parse" and "how do we need to present it" instead of a giant code block that's all tangled together.
-- 2. Tests now can be more complex, test more functionality because it doesn't have to test functions with 30 if statements in them, and actually be written without modifying the real code. Need to start this while things are small...
-- 3. Lastly, every time I say "let me add a new feature", I don't have to "make it fit". I should have a class that defines that functionnal "idea" and where it fits in.
- TL:DR: I didn't need to do this now, but it will be easier now then when the code is already tangled.

#14.1. 4/1/2018:
- Moved a couple classes that I didn't get to yesterday, since the factories imply the internal/implementation details don't need to know. Trust the interface.
- PSA: don't kill yourself coding. It's never worth it. Take it from my own experience: I keep wanting to get a bunch of stuff done, and end up staying up late. Doing that and getting up late, so you get the sleep, is fine (so long as work, school, whatever is your real life allows it). In my case, I have been getting what I mentally averaged to 4-5 hours a sleep a night... for like 6+ months. It's made me more tired, sluggish, harder to focus, and I've gotten more sick then I'm used to. I need to break this habit...
- ...which brings me to: I'm sick and need rest. So I did an additional refactor which got rid of a redudent class, though at the detriment of being able to insert mocks easily... (didn't I write about this yesterday?). I will probably reintroduce the class or an equiv. component.
- For now, I sleep. Tomorrow, I'd like to plan further moves and maybe get a public list of tasks I'm trying to do.

#15.1. 4/2/2018:
- PSA: being sick is annoying. Take care of yourself.
- Today I worked on docs... a needed evil

#16.1. 4/3/2018:
- More doc work. I had a timeline to follow, but it was agressive when I wrote it, and probably can't do such a thing (and ideas for how/what to do also became more concrete between then and now). Simplified:
-- Day 1. figure out program name
-- Day 25. Have a library that can parse a large-ish log quickly
-- Day 50. Initial GUI that can visually show the flow of the logs
-- Day 75. Have a working library for parsing, GUI for visually debugging through the logs, support for RPCs and message tracing, have let others (I know directly) try it out and see what they think and what they want/have issues with, and start working on an offical CLI and search capability
-- Day 100. Version 1.0, with a bit of polish, integration with a real debugger (at the time, I assumed to augment a debugger with log info... but I'm not sure how much that would actually be needed), and possibly work on any stretch goals I could think of along the way.
- Talk about agressive...

#17.1. 4/4/2018:
- Thought a bit and opted to just split the program and library portions. There was no good abstract interface that would allow unit testing AND be usable by more then just one class/program.
- Instead decided to start trying to get unit tests setup, starting with spliting the program and library.
- At least add NUnit reference

#18.1. 4/5/2018:
- I've been struggling with the "dedicate 1 hour" for the 100 Days of Code project. First it was habit, next sickness, now is the disease all software engineers have: time. I look at a clock and say "I have time" and then when I next look, it's time for bed.
- Realized I missed one attribute within the logs I'm testing. Also realized the names of those attributes are not always the best, so added some docs around them.
- Wanted to get the test project setup with at least one test. Didn't like the tutorials for using NUnit within Visual Studio, but found http://www.dotnetcurry.com/visualstudio/1352/nunit-testing-visual-studio-2015 which I liked.
- So I got the test project and what would be test #0... and Visual Studio is complaining about the lib not having a .exe or .dll extension (according to the bin dir, it has a .dll extension). If it was earlier, I'd try to fix it. For now, tests pass

#19.1. 4/6/2018:
- I'm sad... I'm sad because I can't get a simple test project to work. I can and should upgrade to VS 2017, but I fail to see that as a requirement to get these tests working with .Net Core. Nearly every tutorial talks like it's so easy "add this dependency and your done" but know what, I add the dependency and my tests don't show up in the test explorer.
- It doesn't help that I'm half asleep (if you ever meet me IRL, you'll find out I'm perpetually tired. I have a small window where I can focus and get stuff done. Today I used that period to play video games)
- Welp... looks like I get to reset my progress back to 0 because I couldn't get anything done today. I am very unhappy with Visual Studio, NUnit, and the tech results I'm getting back from doing searches. This should've been "Visual Studio wants to run tests, NUnit looked for NUnit references... found them, VS is now happy to show the tests". But this isn't the case.
- I'm not calling it defeat but this is truely annoying. It's stuff like this that makes me drop projects... dependency? Right click dependencies and add to it. Good. Unit test? Seems like a half-hearted attempt to add test support but not maintain consistancy throughout (and I'm a stickler for consistancy). Packaging? I'd like to offer a single sentence response, but there are too many variables for it. Zipping a directory doesn't count, that's a hack.
- Still firm belief this is because I'm really tired.

#0.2. 4/11/2018:
- 4/7/2018: Took a short break, since I'm starting the countdown again. Namely, played around with modding a game I like
- 4/8/2018: Made the mistake of starting late again... today (and probably tomorrow) was spent trying to debug the NUnit issue. Visual Studio doesn't always show the logs for running tests, but I managed to get it a couple times and they all say they can't find "nunit.framework". My challenge now is that I'm looking through the code paths to get to the "error", and the stack traces don't match what is getting printed and what files I have. Reading, nunit (and others?) seems to suffer from VS cache issues that requires deleting the old cache. Tried, didn't work. But given the stack trace and the code I'm looking at, something's definitely wrong.
- 4/9/2018: Did a deep dive into what DLLs are getting loaded. I started too late again, and still have no lead. It's trying ot call NUnit.Engine.Runners.DirectTestRunner.LoadDriver and that just doesn't exist. I cleared caches to ensure it wasn't loading it from a strange location... I don't know where it's getting that function from. The journey continues...
- 4/10/2018: I tried a couple things... and everything has failed. VS UI, command line, extensions, packages, different versions, etc. The only thing left is to upgrade to VS 2017. It's very stupid to me. NUnit with VS support is advertised as "Supported VS 2012 RTM and newer" and yet every test, post, discussion about it is on VS 2017. I'm only one version older... VS 2015, Update 3. I kept putting off the upgrade due to laziness AND not having a need. The posts all speak of the "new" XML-based project configs making things so much easier(?). It's all very stupid and a waste of time. In fact... it's a perfect reason tests exist. At work I aim for 100% branch coverage because I'm crazy... but when I get told "your code doesn't work" I can respond back "no, you did something wrong". Then I (or a coworker) goes and automates them. When I look at the NUnit code, I see a handful of tests for the adapter. It makes me wonder "did anyone try automating the tests for VS 2012, VS 2013, VS 2015, and VS 2017 at the same time? Or did they just run the tests after telling all other contributors that they were working in VS 2017 and called it a day? But... enough of rants. Let me get VS 2017 so tomorrow I can see if I can actually DO something, and get this process going again. If it still doesn't work, I'm dropping NUnit, sending a message to the devs (and making a bug), and probably moving to xUnit or similar. No need to waste any more time. I needed to start a new attempt because tests didn't run, and from what I can tell, it's the framework/adapter that isn't working. "Famous last words" but if I upgrade to 2017 and it magically works... well, was my assumption wrong? For tomorrow.
- So after a lot of pain, needing to restart my count, and run upgrade to VS 2017... tests actually run.
- So, https://github.com/nunit/docs/wiki/.NET-Core-and-.NET-Standard half tells you what to do. I followed https://docs.microsoft.com/en-us/dotnet/core/testing/unit-testing-with-nunit via CLI and some manual editing and got everything working as I'd like.
- Looking through docs now, I see the key line I missed: "FAQ -> Why can't my tests target .NET Standard? -> "...it cannot be .NET Standard, it must target a platform, .NET Core or .NET Framework."
- See, if you look at my log for #5.1, I mention .Net Portable Standard. Diving into why tests aren't working, I have since learned that .Net Standard is equiv of an interface (IDotNet), and .Net Core the impl. (DotNet). I didn't know what the difference was or why things weren't working. I am both right and wrong with what I said yesterday... the framework didn't work... but the devs don't know why and are hoping it will one day work. They blame Microsoft for wanting a hard impl. of .Net (.Net Core or .Net Framework). In the end, it was a giant waste of time. I wonder if there is some way to determine at runtime what a .Net test project is running under (Core, Standard, or Framework) and say "hey, you can't run as Standard. Needs to be a different base". It's probably something that someone has done and said "this is too much work... just write the docs" but, as is typical for docs, is in a bad location (it should really say that here: https://github.com/nunit/docs/wiki/Known-Problems) and people all it a day. Well, now I know and hopefully if someone looks at my project and has the same issue, they eventually find this log and go "yea, that should be written in a different location". I'm not exactly doing anyone a favor by not contributing a fix... but I honestly don't know if that location is the right place. It's just the one I kept coming across.
- Right as I was about to start, I noticed that the libraries have an attribute compiled into them that states the framework used. So my automated version _could_ be done. Have the test running get assembly attributes, find the framework one and, if it exists, check if it's a framework that it knows about/can use. If not, then error with a sane message. I'll note this as something to possibly contribute to NUnit at another point of time.
- I can now start working on all the other tests tomorrow.

#1.2. 4/12/2018:
- I started off tests by... reading. RTFM. If I didn't, I would've just did the "classic model" of "Assert.Equal(X, Y)" which is not what NUnit prefers. So I did a bunch of reading...
- ...and redid the existing test.
- I started late, so I didn't get to anything else. Oops. Time to get back into the habit.

#2.2. 4/13/2018:
- Wanted to spend more time on this, but I need to be up early.
- I was trying to figure out what to write a test for next and realized I had defined an ILogEntry interface, used it in ILogRegistry, but... the actual GetBy function (basically a DB/dictionary query) had hardcoded LogEntry instead of ILogEntry. So that needed to change.

#3.2. 4/14/2018:
- So I always start late and commit on the following day then the one I log for. I will probably still do that, but as I had to do stuff early yesterday, I need to commit today and "tomorrow" so it doesn't appear I skipped a day.
- Started working on more tests. First up: factories. Gave me a reason to add a mocking library (NSubstitute) and to use it. I need to read the docs a bit. I'm used to gmock/gtest, where I expect something to be called and then it complains when it is called (and wasn't expected) or isn't called (and was expected). NSubstitute seems to work off a "check after the fact" but won't tell me if something got called unexpectedly. I feel like that isn't actually the case, but I haven't gone through the logs (or written something that does that). Yet.
- Started working on LogEntry tests.

#4.2. 4/15/2018:
- More tests time. I actually gave some thought and did TDD for the remaining tests in LogEntryTests. So now things work as expected.
- Next up: LogRegistry tests. I realized I don't have a LogEntry factory and question if I need one. I can see different parsers and printers... but what's a different log entry? Yes, I can unit test it slightly easier, but at the expensive of adding a whole new factory. Given how easy it would be to add (add interface with a create function, pass into registry constructor, use...), I'll skip it and can add it if needed at a later time.
- TDD time again... this is a habit I need to get into in general. At my work, it's always easy to spend a day or two working on a class, then spend a few hours on tests and know all the paths to test. But the point is to write how you expect the program to work, not how you know it already works.
- I got a couple tests, only to realize my implementation work as I expected. Initial thought was "what am I doing wrong", then looked at the impl. and realized "I don't want that..." (getting logs by their message or timestamp). See the "todos" in LogRegistryTests.cs
- I also made the mistake of doing too much in a unit test, and it didn't work because of my GetBy impl. anyway

#5.2. 4/16/2018:
- Wrote a lot more LogRegistry tests. Half TDDed, half "was updating something in LogRegistry when I noticed a case I coded to handle, but didn't write a test for".
- The big question for this evening is "return error, or exception?" So this has been a struggle recently for me. All the time I did C# (Java, F#, and others), throwing an exception "is what you do" (TM). Then along comes C/C++. C++ has exceptions, but I've become a bit more "I want a good, clean, end-to-end opcodes" when doing native development. As such, I've seen how Windows and Linux handle exceptions, and it's not pretty. I've also learned that "-O3 optomized" tends to not mean squat if your code isn't in a specific format. Also, C++ is a lot easier to forget to cleanup a resource that was mid-process. Think:
int main()
{
    try
    {
        myFunc();
    }
    catch
    {
    }
    return 0;
}

void myFunc()
{
    std::ifstream in("log.txt", std::ios_base::in);
    doSomethingCool(in);
}

void doSomethingCool(std::ifstream& input)
{
    //...
    throw new std::exception("oops");
}
- I'd expect that the input stream would cleanup. But... I don't know. I'd like to not know, because it would make programming easier (I.e. assuming that stuff gets cleaned up for you). It's not naive, it's taking the human element into account when designing a language.
- Meanwhile, in my preferred native language: C... what's an exception?
- So with plans to move this to C++ at a later point, I had to decide: do I write C# functions without exceptions or not?
- My answer was: depends. If what I was dealing with something had a return status (either from the function itself, or as a different function), I didn't throw an exception. If it didn't, then I threw the exception.
- The way I figure it: if I'm saying "add this value to the log entry" and I want to know if it got added, instead of getting said value and comparing, I'll just look at a return code. So for all failures, return according to return code. Else, I have no idea I did something wrong... so throw the exception.
- Bonus: unrelated to ^^, I added a GetByTimestamp function to LogRegistry and changed the return from GetBy from IDictionary<object, IEnumerable<ILogEntry>> which allows me to support getting by log message* AND still works in pretty much any way I expect.
- * so, to make a grouped dictionary, I need to go through every single log...  working with yield would be better but it's still going to be very challenging to deal with large logs.

#6.2. 4/17/2018:
- I wanted to get many things done today... and it came at the fall of the unit tests.
- I didn't miss a commit or need to start the countdown again, but plans will have to be tomorrow.
- Instead I added a couple more tests and ensured LogEntry supports Equals, GetHashCode, and ToString.

#7.2. 4/18/2018:
- I typo-ed dates in my logs. I was panicking for a moment as I thought I somehow missed a day of work.
- Spent most of the time trying to see if I could get code coverage. It's something I look at work, possibly too much. Not because I'm required to, but because I want to make sure if I do anything. If I sneeze and hit a key on my keyboard, I don't cause errors. True story (sneezed, hit a key while typing a string. Compile worked, but runtime failed. Eventually found I was looking for the wrong input file). So, I write a lot of tests and want to make sure everything is covered.
- Long story short: maybe in the future, but not now. VS2017 Community doesn't support code coverage. OpenCover looks nice, but I need to make sure to have the correct command line. NUnit keeps crying it can't find nunit.framework and Google points to semi-relevant responses, but none of them "NUnit cannot find framework" so I couldn't do anything without spending much too long on it. Instead, I looked and found "dotnet test" will do the work without a hitch... except the only output format it supports is trx... and OpenCover doesn't have a clue what to do with those. And NUnit has had a year-long ticket open for adding support with exporting the test results via XML in the VS adapter. So it's one step foward and one step back.
- Planned to do some additional work, but will push it to tomorrow.

#8.2. 4/19/2018:
- Suddenly, nap. Thus: instead of doing the same task I wanted to do yesterday, I did something else: testing Equals, GetHashCode, and testing ToString(?) for LogEntry
- I tried something a bit different in I used a TestCaseSource input... then vastly overcomplicated it (see the code for why. If you don't see a complex test impl., yay?)
- I also learned about "TestCaseData" and wonder if I can/should use it. Right now I say "maybe" but will get tests working first.
- All this to get as much test coverage (sans using a tool that tells me the test coverage), so that as I do more work, I can ensure everything is working AND adding new tests is simply adding a test case rather then a whole writeup. Word of advice, taken from work experience (and it's written about enough in motivational blog posts): just start. In the case of unit tests, if I wrote everything and said I'll do it "eventually", it will never happen.
- The new challenge: VS/NUnit has decided that the 2 "Equals" tests I wrote for LogEntry are weird and did the following: made a warning/error in console about "converting" the tests, ran all the tests and said they passed, told me the two tests "did not run", told me the whole test suite didn't pass. Something's weird as I managed to get "pass", "did not run", and "fail" for the same tests at the same time.

#9.2. 4/20/2018:
- Ignore starting working on this late, I had another issue: God of War came out. Between that, Fortnite, and Parkitect (my current game collection), and a little thing called "life", I watched 8 hours fly by in an instant. This project must still go on... but a challenge:
- So I want to cover as many cases as possible. An ideal in programming, to me, is that something "just works" or works as expected. So if I create a LogEntry, I expect that I can stick them into a list. I expect I can "simply" print them out. I don't expect to use some custom formatter, LogEntryList, etc. In .Net, that means supporting ToString, GetHashCode, and Equals.
- C# makes it nice. Override those three functions, and .Net will keep running without a hitch. Don't override them and... things will still work, but you will hit cases that what you expect doesn't work, or that something might not work at all. Output a #6.2 version of the library to a GUI List and you'll find every entry is "LogTracker.Log.LogEntry" instead of a unique log message.
- Sets, Dictionaries, comparisions, ==, and many others all have the same challenge: they use one of those functions.
- But there's a different challenge: C++. I will continue to be adamant about this eventually being written in C++. C++ doesn't have the niceities of C#, so when I say "give me a set of LogEntries", it's going to ask me for something to print out the hashes of each LogEntry or hope the default works.
- So I need the values... AND I need them to work. That last part is where all the tests come into play. I want to test as many potential cases as I can without going crazy. (At work, our QA team has been working to do more "orthogonal testing" so it's not writing hundreds of tests).
- There were 3 sets of tests: non-LogEntry, LogEntry, and "complex" LogEntry. The first were accomplished with a simple array. The second required more details... and I wrote "TestObject" to be a nice wrapper for them. But when I've gone into the complex cases, it got, well, complex.
- I half wrote updates before realizing the test is not going to work as expected and will require a fair amount of rewriting. Instead, I'd rather figure out a good way to write these tests (that blends with NUnit?) so I can do my three sets of tests, but without the test itself going "easy, intermediate, hard" to implement.
- Doesn't help that I might have to do this 3 times: Equals, ToString, and GetHashCode.
- Bonus: I may have figured out an issue with tests yesterday. NUnit (or Visual Studio) use names to define tests. My helper class did a printout of LogEntry, which prints a date-time. Suddenly, all my tests pass. But then I run again a little later, and it's a _new_ set of tests, and some generic case claims it didn't run. BUT WAIT, there's more: I... scrolled down. I never scrolled up to see a couple tests were failing in general. That has been fixed, but the others haven't (and I haven't tested tests with the same name, but different classes, to see if they all show up or only one).
- For tomorrow...

#10.2. 4/21/2018:
- Yay, back to double digts in progress.
- I did a full rewrite of the Equals tests. Now with a builder for the test data
- "A BUILDER?!" says a set of devs... yes, a builder. The builder code is slightly longer then the "still in progress" TestObject (since deleted), but instead of `TestObject(arg, arg, arg, arg, arg, arg, arg, () => fancy work)`, it's closer to an explanation of what the heck it's doing.
- Also, the builder outputs a TestCaseData, which has it's own set of functions. So I'm able to utilize it to do additional work for me.
- Long story short: I can do more complex tests with less work. Only hacky element is adding attributes to the LogEntry to test against... it was either that, or make the whole thing Lazy and that would just be ugly.
- Minor refactor so test helpers are located elsewhere
- Now... fix the Equals functions so the tests pass

#11.2. 4/22/2018:
- A quick test laid to rest that "same name tests won't be shown as seperate tests". They show up with the same name, but indicate they have different sources.
- Attempted to setup GetHashCode tests... but a couple tests are failing...
- Attempt #2... this time, return the actual hash code. Somehow, the first attempt had 2 failed tests. This one has 8 failed tests. I don't know if testing hash codes is a thing. I guess it's worth testing, but I also worry about how Dictionary GetHashCode works. If it's implemented somewhere, I haven't found it. I worry it's producing some random results for me (attribute order?) but regardless, this isn't working and I can't figure what went wrong. So I'm going to trash it.
- Realized I test two attributes that are tested a second time later. So changed the Equals operation to get the values directly from the attributes, and to test the other values.
- I opted not to do a ToString, because the Printers will be doing the work, not the ToString.
- Now to ginally get back to what I really wanted to work on: TDD. I want to write all the remaining tests, without running them and without fixing the failures. Then test and fix failures...
- ...and I got 1.5 of 5 tests in before I need to call it quits. Sigh.

#12.2. 4/23/2018:
- Finished GetBy tests for LogRegistry. Technically still have to make tests for the actual implementation, but that's for another time.
- Nevermind. I wrote the remaining tests.
- Next up: ConsolePrinter. I want to test this over IOPrinter because it will actually be used. If something gets changed in IOPrinter, suddenly there will be X tests breaking, where X is every implementor of IOPrinter.
- Actually, writing that, I probably should do it for IOPrinter... Or at least have some tests "for" IOPrinter derived classes, but use the implementation (IOTester.Test(impl, outputGrabberDelegate)). This way I can run the tests on the impl, which will actually get used directly, and NOT have to write 10 tests the same way.
- First tests are... sanity tests. What if there are no logs, and just make sure I can grab console output.
- Both work, but the console output test is... not plesent. Can you imagine every test needing to check the NewLine? Probably be best to specify a new line...
- But wait, there's more! I was under the impression that NUnit ran in parallel (at work, we started non-parallel, then wanted tests to run faster, switched to parallel, and watched many break).
- As such, I wrote all the tests (at least I think so) with the expectation that they would run in parallel. Since the ConsolePrinter test would be changing the global Console.Out param, I couldn't have that run in parallel.
- I marked the test as NonParallelizable. Curiosity, I looked into it and... apparently NUnit doesn't run in parallel by default. Probably a good default, but it now meant that I wanted everything to run in parallel (unless marked).
- You can apparently mark assemblies, classes/fixtures, and methods as Parallelizable. I didn't want to go nuts right now, so I marked only fixtures (at an assembly level) as parallelizable.
- But there was no place to put the attribute. Generating the test project apparently meant it would generate the assembly info at build time... but left me without a place to put assembly level attributes. Luckily, diff tools to the rescue.
- There are attributes that tell the project not to generate the assembly info, and that left me with the ability to write my own.
- All is good in the world. Next up: writing the rest of the ConsolePrinter tests, and doing the work mentioned above.

#13.2. 4/24/2018:
- Specify our own newline, so changes in platform are easier to handle
- Abstracted IOPrinter tests
- Started working on IOPrinter tests... this is going to be either annoying, or complex.
- ...and would you look at the time. *cough* Yea, it's late (again) and I want to be able to focus on this then to be half-asleep while trying to ensure print outs occur as expected

#14.2. 4/25/2018:
- Started off trying to finish the IOPrinter tests. Made good progress, if not completed it.
- And... not feeling good. Occured just after I ate some food... that can't be good. I'm gonna have to call it a night early (unintentionally this time)
- One more, different sources

#15.2. 4/26/2018:
- Late day... probably should've done something before playing games.
- So right before I finished yesterday, I saw I had opened a tab about GetHashCode and it, like StackOverflow and others, basically said: if Equals is true, GetHashCode should too (hey, that rhymes).
- I added a GetHashCode test to the equals tests. If it's not equal, it won't test hash code. If equals, it will test hash code.
- After a short thought, I will add the GetHashCode tests, but they're simply going to be comparing the values. The && I was doing in the equals tests would mean that "equals = T && hash code = F" would tell me the test failed, but I wouldn't know where. Likewise, "equals = F && hash code = T" would mean we can have a colision between LogEntrys, but would never know because the && made it false. So a seperate test would be better. Will do this another time.
- I would've liked to start implementing the tests for ParserUtil (at least the InternalsVisibleTo is working), but as stated: it's late. And stated at another time, don't kill yourself over code. I haven't been too good at that lately. Make myself too busy.

#16.2. 4/27/2018:
- I'm still standing. Somehow. I could only spend a portion of time on this, so I started implementing test cases for ParserUtil.
- I realized part way through that I could use TestCases (instead of test) to vastly increase the number of tests without making the test file giant.
- (You don't realize the difficulty it was to write ^^. I need to go to bed. Late night D&D didn't help with being tired)
- For some reason, doing invalid tests resulted in CS0051 coming from C#. I tried changing the values, but C# continues to complain. When I didn't do TestCase, I didn't get errors... but I also didn't try compiling, so it may have happened there.
- Regardless, this kind of work should be easy. Just need to _not_ be asleep.

#17.2. 4/28/2018:
- Fixed the CS0051 issue with a test
- Added the string and int tests for casts.
- Was preparing to work on Key-Value tests, but realized it would be a bit more work then I have (the joys of working on this, late, again). I will do this tomorrow and start on testing the parser itself

#18.2. 4/29/2018:
- Right after writing yesterday's note about Key-Value tests, I realized "this is basically the same as the LogEntry tests, but with a different expected data. Instead of writing a whole new builder, I should just extend the other one."
- This was followed closely with "Wait, even better. Common logic in a parent class, then simplified top-classes". Then I was done for the night.
- I went about to do just that... and basically wrote a whole new builder. The abstractions are useful, as it will make it easy to create more advanced tests (I'm imagining XML parsing tests, and maybe GUI work too? We'll see when the time comes). The basic need is for tests that produce non-const data AND takea variable input.
- Simple input or complex setup means needing individual test cases. But complex input can't be compiled in, so it would need individual tests. But then it's a lot of copy-paste and/or a helper function to do the work. The TestCase attributes simplify testing.
- Casting Key-Value tests needed some additional helpers to make these work better. Also, adding an extension class made me very happy... because it meant that I didn't need to have some ever expanding base class AND it meant I could now do something fancy (see TestDataBuilderExtensions.For and explain how to do that in a base class. As in, C# syntax won't allow you to have a base class do some action and return a child class. But it can do it as an extension function)
- Now... the tests I came up with. Most are obvious, the last couple are going to be a pain. I put them in because I can easily see logs coming in that go "uuid=XXX-XXX-XXX; name=Smith; userData={\"name\":\"John Smith\", \"userNotes\":[\"I'm debating on if the data should be A;B;C or A,B,C. I need the result to be key=value and...
- ...that isn't going to be pretty doing a naive split right now. I'd rather the parsers NOT have to have to handle the complex cases individually. Instead, do it in the cross-parser utility classes.
- In addition, in keeping with "TDD" (notice the quotes, since I haven't been doing to good with it), I can plan these cases now and worry fix implementation later.
- It also sets up later implementations/tests such as "implicit parsing" (I don't have a better name). At some point in the future, I'd like to be able to get log data and, if the log data didn't have a specific type or there was a type mismatch, have the system determine the data type ahead of time.
- A parser (utility) that can split data without mixing up quoted data means that I can take the sample from above and do an implicit parse of the userData to JSON.
- I'm getting ahead of myself. I've spent a long time on making UT. I hope it all pays off in the end, so I can build and know all the old code still works.
- Also, these new tests broke the 100 tests count.
- I didn't get to the parser testing or ParsePath tests, but I hope to get to those within the coming days (one of those days will be catching up on movies, the other will be watching Infinity Wars. So I expect both of those to be slightly smaller updates in an effort to get to get enough sleep)

#19.2. 4/30/2018:
- So when I said "smaller updates", it ended up being very small.
- I started looking into the string split. This seems to be a perpetual problem with strings. Or at least I always seem to encounter "split a string, but if there's a quote inside the string, don't split on that"
- This might be something I spend one day on, then leave it for another time.
- In the mean time, I fixed the null case and added an additional seperator key (gut feeling is this should be a config value)
- I also added ever increasingly complex cast key-value tests...
- Note: I looked at https://stackoverflow.com/questions/554013/regular-expression-to-split-on-spaces-unless-in-quotes and it looks good, now I just need to use different seperators
- Note 2: These are the test strings that are failing because of quotes (without the C# escaping):
1. "key=value"="test=pain1"
2. "key=value;oh=boy"="test=pain1"
3. "key=value;oh=boy"="test=pain1";yep="this=pain2"
4. "let's \"use=more\" quotes"="oh boy"